---
title: "Data, Power, and Consent in the DeepMind–NHS Streams Partnership"
format: html
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

In 2015, the Royal Free London NHS Foundation Trust, a large public hospital group within the United Kingdom’s tax‑funded National Health Service (NHS), partnered with Google DeepMind, an artificial‑intelligence lab owned by Alphabet, to develop Streams, a smartphone app intended to alert clinicians in real time when patients showed signs of acute kidney injury. To build and test Streams, DeepMind received access to the medical records of approximately 1.6 million NHS patients, including highly sensitive information such as HIV status, mental health history, and drug‑use data. (Powles & Hodson, 2017).

What began as a promising digital‑health initiative quickly became one of the UK’s most controversial data‑science collaborations when it emerged that patients had not been told their records would be shared with a private company and had not consented to this use. In 2017, the UK’s Information Commissioner’s Office (ICO), the national regulator for data‑protection and privacy, concluded that the partnership violated data‑protection law because the data had been shared under the justification of “direct patient care,” even though Streams was still being tested and was not yet part of routine care.(University of Cambridge, 2019).

This partnership became a defining example of how ethics, power, and corporate interests intersect in artificial intelligence research. It showed that technological innovation in healthcare cannot be separated from the moral questions surrounding ownership, consent, and the use of personal data.

## 1. Consent and Permission Structures

**Questions addressed:** What was the permission structure for using the data? Was it followed? What was the consent structure for patients? Were they aware of how their data would be used? Is informed consent possible for unforeseen future applications?

Under the UK Data Protection Act, organizations must have a valid legal basis for processing personal data. The Royal Free Trust argued that sharing data with DeepMind was justified under “direct patient care,” the same basis that allows clinicians to access records to diagnose and treat individuals. However, Streams was in a research and testing phase; patients were not yet receiving care through the app, so this legal justification did not apply.

In 2017, the ICO found that the Royal Free Trust had failed to adequately inform patients about the partnership or the ways their data would be processed. Patients were not told that a private AI lab would receive their records, nor were they given a meaningful chance to opt in or out. Genuine informed consent requires that people know who will access their data, for what purposes, and with what potential consequences. In this case, patients had no realistic way to understand that their records might support algorithm development, later products, or broader commercial AI strategies.

More broadly, the case highlights the limits of conventional consent models in big‑data healthcare. Fan, Zhang, and Li (2025) argue that when AI enables open‑ended reuse of data, future applications are often impossible to foresee at the point of collection. The DeepMind collaboration demonstrates how quickly clinical data, obtained for treatment and operations, can be repurposed for research and development far beyond what patients initially understood.

## 2. Data Privacy and Identifiability

**Questions addressed:** Is the data identifiable? Are parts of it sufficiently anonymized to avoid ethical concerns? Is anonymity realistically guaranteed?

DeepMind received rich, line‑level health records from the Royal Free, including names, addresses, dates, and detailed medical histories. Although the organizations stressed that the data were stored and processed securely, the ICO concluded that the scale and identifiability of the dataset went well beyond what was necessary for testing Streams.

Even where explicit identifiers are removed, true anonymity in health data is difficult to achieve. Murdoch (2021) notes that combinations of diagnoses, dates, demographics, and geography can re‑identify individuals when linked with other datasets. A small set of quasi‑identifiers, such as age, postcode, and rare conditions, can be enough to single out a person.

In the Streams case, DeepMind’s access to identifiable or easily re‑identifiable records without explicit patient consent represented a clear privacy risk. It exposed weaknesses in data‑governance processes around data minimization (only using what is strictly necessary), independent oversight, and clarity about who is accountable when public providers share health records with private AI developers.

## 3. Representation and Bias in Algorithm Design

**Questions addressed:** Who was measured? Are these patients representative of the broader populations where the algorithm might be used? Should we analyze data if we know the sample is narrow?

The training data for Streams came from a single hospital trust in London, which limited the demographic and clinical diversity of the dataset. Algorithms built on such a narrow population may not perform well for patients in other regions, health‑system contexts, or with different ethnic and socioeconomic backgrounds.

Dankwa‑Mullan et al. (2024) emphasize that AI tools trained on unrepresentative data can reproduce and intensify existing health inequities, especially when they are deployed at scale without rigorous external validation. D’Ignazio and Klein’s (2020) Data Feminism framework similarly stresses examining who is included and excluded in a dataset, and how those choices reflect existing power structures and systemic bias.

Because Streams was trained primarily on patients from one trust, its fairness and reliability for other NHS populations were uncertain from the outset. Yet the project attracted national and international attention as a model of “AI for healthcare,” illustrating how quickly tools based on local data can be implicitly generalized to much broader contexts.

## 4. Data Use Beyond Original Intent

**Question addressed:** Is the data being used in ways that go beyond the purposes for which it was originally collected?

The NHS collects patient records to deliver care, manage services, and meet legal obligations, not to provide a general data resource for private AI labs. In the DeepMind partnership, however, information originally gathered for clinical purposes was repurposed to develop and test a machine‑learning system owned by a subsidiary of a large technology company.

DeepMind described Streams as a non‑commercial patient‑safety project, but Alphabet nonetheless benefited from technical expertise, infrastructure, and potential intellectual property generated through the collaboration. Fan et al. (2025) argue that in such arrangements, public institutions often provide the data while private firms capture most of the long‑term value in the form of algorithms, know‑how, and future products. Patients whose data made the work possible rarely participate in decisions about reuse and typically do not share in any commercial benefits.

From a Data Feminism perspective, this reflects a structural power imbalance between public healthcare systems and corporate technology actors. Public providers depend on large technology firms for AI tools and infrastructure, while those firms gain influence over public infrastructure through control of data and models. The Streams case shows how easily public health data can be drawn into private research agendas once consent and governance safeguards fail.

## 5. Why It Matters

**Questions addressed**: “Who benefits? Who is neglected or harmed? Were the ethical violations in the interest of profit, surveillance, or power?”

In principle, tools like Streams can improve patient outcomes by flagging kidney injury earlier and helping clinicians intervene more quickly. In practice, the way this collaboration was structured concentrated benefits and risks unevenly. Alphabet gained access to a large, high‑quality dataset and an opportunity to develop AI capabilities in a high‑profile clinical setting. Patients, by contrast, lost control over sensitive information and were exposed to privacy risks without their knowledge or consent.

Because the data came from one trust and patients had no role in governance, the project also risked reinforcing inequities; improving care for some groups while leaving others unserved or mis‑served. As D’Ignazio and Klein (2020) argue, ethical data science is not only about meeting legal requirements; it also demands attention to fairness, accountability, and the protection of those with the least power.

The DeepMind–Royal Free case ultimately serves as a cautionary tale. Inadequate consent, insufficient anonymization, narrow training data, and repurposing of patient records for corporate AI development all reveal how easily technological innovation can outpace ethical and regulatory safeguards. Without stronger consent mechanisms, transparent governance, more representative data, and fair sharing of benefits, AI in healthcare risks reproducing the very power imbalances and inequities it claims to address.

## References / Data Source 

Dankwa-Mullan, I., Nunez-Smith, M., & Volerman, A. (2024). Health equity and ethical considerations in using artificial intelligence. Centers for Disease Control and Prevention. <https://www.cdc.gov/pcd/issues/2024/24_0245.htm>

D’Ignazio, C., & Klein, L. F. (2020). Data feminism. MIT Press.

Fan, X., Zhang, L., & Li, Y. (2025). Exploring the ethical issues posed by AI and big data in healthcare. Frontiers in Public Health, 13, 1585180. <https://doi.org/10.3389/fpubh.2025.1585180>

Murdoch, B. (2021). Privacy and artificial intelligence: Challenges for protecting health data. BMC Medical Ethics, 22(122). <https://doi.org/10.1186/s12910-021-00687-3>

Powles, J., & Hodson, H. (2017). Google DeepMind and healthcare in an age of algorithms. Health and Technology, 7(4), 351–367. <https://doi.org/10.1007/s12553-017-0179-1>

University of Cambridge. (2019, July 16). Royal Free London NHS Trust–Google DeepMind deal is “cautionary tale” for healthcare in the algorithmic age. <https://www.cam.ac.uk/research/news/deepmind-royal-free-deal-is-cautionary-tale-for-healthcare-in-the-algorithmic-age>
